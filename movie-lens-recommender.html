<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kim Wager">
<meta name="dcterms.date" content="2025-05-23">

<title>MovieLens Project Report</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="movie-lens-recommender_files/libs/clipboard/clipboard.min.js"></script>
<script src="movie-lens-recommender_files/libs/quarto-html/quarto.js"></script>
<script src="movie-lens-recommender_files/libs/quarto-html/popper.min.js"></script>
<script src="movie-lens-recommender_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="movie-lens-recommender_files/libs/quarto-html/anchor.min.js"></script>
<link href="movie-lens-recommender_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="movie-lens-recommender_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="movie-lens-recommender_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="movie-lens-recommender_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="movie-lens-recommender_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="movie-lens-recommender_files/libs/kePrint-0.0.1/kePrint.js"></script>
<link href="movie-lens-recommender_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MovieLens Project Report</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kim Wager </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 23, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<section id="recommendation-systems" class="level3">
<h3 class="anchored" data-anchor-id="recommendation-systems">Recommendation systems</h3>
<p>Recommendation systems use ratings provided by users to generate personalized suggestions. Organizations like Amazon, with extensive product catalogs and customer bases, accumulate vast datasets of user ratings. These large datasets can be used to train models that predict how a specific user might rate items they haven’t seen before. Items predicted to receive high ratings from a particular user are then recommended to them by the organization to drive sales.<span class="citation" data-cites="irizarry2024">(<a href="#ref-irizarry2024" role="doc-biblioref">Irizarry 2024</a>)</span></p>
<p>In the entertainment industry, Netflix uses recommendation systems to predict user movie ratings on a five-star scale, where one star indicates a poor film and five stars represents an excellent film.<span class="citation" data-cites="irizarry2024">(<a href="#ref-irizarry2024" role="doc-biblioref">Irizarry 2024</a>)</span> The objective of this project was to develop a machine learning model to predict movie ratings.<span class="citation" data-cites="Re3data.Org2016">(<a href="#ref-Re3data.Org2016" role="doc-biblioref">Re3data.Org 2016</a>)</span></p>
</section>
<section id="the-movielens-dataset" class="level3">
<h3 class="anchored" data-anchor-id="the-movielens-dataset">The MovieLens dataset</h3>
<p>While Netflix’s proprietary data are unavailable, the GroupLens research laboratory has developed an alternative open-source dataset called MovieLens with over 20 million ratings for more than 27,000 movies, provided by over 138,000 users.<span class="citation" data-cites="Re3data.Org2016">(<a href="#ref-Re3data.Org2016" role="doc-biblioref">Re3data.Org 2016</a>)</span> A subset of these data (MovieLens 10M) were made available for this project via the <code>dslabs package</code>.<span class="citation" data-cites="dslabs:2017">(<a href="#ref-dslabs:2017" role="doc-biblioref"><span>“Dslabs: Data Science Labs”</span> 2017</a>)</span></p>
<p>The MovieLens 10M dataset contains approximately 10 million ratings applied to 10,000 movies by 72,000 users. The dataset includes user IDs, movie IDs, ratings (0.5-5 stars), timestamps, movie titles, and genres. Each movie can belong to multiple genres (e.g., “Comedy|Romance|Drama”).<span class="citation" data-cites="Zhuo2024">(<a href="#ref-Zhuo2024" role="doc-biblioref">Zhuo 2024</a>)</span></p>
</section>
<section id="project-goal" class="level3">
<h3 class="anchored" data-anchor-id="project-goal">Project goal</h3>
<p>The goal of the project was to develop several movie recommendation models to develop a system that accurately predicts how users will rate movies they haven’t seen yet. To compare the performance of the models I used the Root Mean Square Error (RMSE) metric.</p>
<ul>
<li>Target: Achieve RMSE &lt; 0.86490 on the final holdout test set</li>
</ul>
<!-- Final I got was RMSE0.8653 -->
</section>
<section id="key-steps" class="level3">
<h3 class="anchored" data-anchor-id="key-steps">Key steps</h3>
<p>I implemented five key steps to develop and evaluate the movie recommender model:</p>
<ol type="1">
<li>Data preparation and partitioning into training, validation, and test sets</li>
<li>Progressive model building from simple baseline to complex models</li>
<li>Feature engineering to incorporate movie, user, and genre effects</li>
<li>Regularization to prevent overfitting</li>
<li>Final model evaluation on a separate holdout test set</li>
</ol>
</section>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">2. Methods</h2>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">Data preparation</h3>
<p>First, I downloaded and extracted the MovieLens 10M dataset according to the course instructions provided by edX. An essential next step was to join the ratings data with movie metadata. This process created a comprehensive dataset containing both user ratings and detailed information about each movie.</p>
<p>To perform the joining of the datasets, I loaded two separate data files:</p>
<ol type="1">
<li><p><code>ratings.dat</code> - Contained user ratings with columns: userId, movieId, rating, and timestamp</p></li>
<li><p><code>movies.dat</code> - Contained movie information with columns: movieId, title, and genres</p></li>
</ol>
<p>After loading and formatting these files into data frames, I performed a left join operation using the <code>movieId</code> column as the common key:</p>
<p><code>movielens &lt;- left_join(ratings, movies, by = "movieId")</code></p>
<p>This join operation merged the two datasets together, keeping all rows from the ratings dataset and matching them with corresponding movie information from the movies dataset. The resulting <code>movielens</code> dataset contained all original rating information plus the additional movie metadata (title and genres) for each rating entry. This unified dataset served as the foundation for all subsequent data partitioning (into training, validation, and test sets) and model development.</p>
</section>
<section id="data-partitioning" class="level3">
<h3 class="anchored" data-anchor-id="data-partitioning">Data partitioning</h3>
<p>To ensure best practices in machine learning, data partitioning maintained separate datasets for training, validation, and final testing to get unbiased performance estimates. First, I created two distinct datasets by using <code>set.seed()</code> for reproducibility and the <code>createDataPartition()</code> function from the <code>caret</code> package:</p>
<ul>
<li><p>90% for model development (<code>edx</code>)</p></li>
<li><p>10% for final evaluation (<code>final_holdout_test</code>)</p></li>
</ul>
<p>The <code>edx</code> dataset was further partitioned into:</p>
<ul>
<li><p>Training set (<code>edx_train</code>): Used to build and train the models (80% of the <code>edx</code> data)</p></li>
<li><p>Validation set: Used to tune hyperparameters and compare model performance (20% of the <code>edx data</code>)</p></li>
</ul>
<p>The final holdout test set was reserved exclusively for final model evaluation.</p>
<p>Following these partitioning steps, I ensured data integrity by using a <code>semi-join</code> to ensure all users and movies in the test sets also appeared in the training set, preventing “cold start” prediction scenarios: any removed rows were added back to the <code>edx</code> dataset.</p>
</section>
<section id="modeling-approach" class="level3">
<h3 class="anchored" data-anchor-id="modeling-approach">Modeling approach</h3>
<p>In summary, I implemented a methodical approach to model development by progressively adding complexity:</p>
<ol type="1">
<li><p><strong>Baseline model</strong>: Used global mean rating for all predictions</p></li>
<li><p><strong>Movie effects model</strong>: Added movie-specific bias terms</p></li>
<li><p><strong>User effects model</strong>: Added user-specific rating tendencies</p></li>
<li><p><strong>Combined model</strong>: Integrated both movie and user effects</p></li>
<li><p><strong>Regularized combined model</strong>: Added regularization to prevent overfitting</p></li>
<li><p><strong>Regularized genre effects model</strong>: Included genre-specific biases</p></li>
</ol>
<section id="baseline-model" class="level4">
<h4 class="anchored" data-anchor-id="baseline-model">Baseline model</h4>
<p>This is a very simple model that ignores individual differences between movies and users and predicts the same rating (global mean, μ) for every movie-user combination. It serves as a baseline that more sophisticated models should improve upon. To implement this model, I first calculated the global mean rating (μ) from the training data and then created predictions by repeating this mean for every rating in the validation set using <code>rep(mu, nrow(validation))</code> to create predictions. To validate the model, I defined an RMSE (Root Mean Square Error) function to measure prediction accuracy on the validation set.</p>
<p>Mathematically, RMSE is represented as:</p>
<p><span class="math inline">\(\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}\)</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(n\)</span> is the number of predictions</p></li>
<li><p><span class="math inline">\(y_i\)</span> is the actual observed value</p></li>
<li><p><span class="math inline">\(\hat{y}_i\)</span> is the predicted value</p></li>
<li><p>i is an index that runs from 1 to n (the total number of predictions)</p></li>
<li><p>The summation (∑) adds up the squared differences for all predictions from i=1 to i=n</p></li>
</ul>
</section>
<section id="movie-effects-model" class="level4">
<h4 class="anchored" data-anchor-id="movie-effects-model"><strong>Movie effects model</strong></h4>
<p>This model improved upon the <a href="#baseline-model">Baseline model</a> by accounting for movie-specific variations, recognizing that some movies are consistently rated higher/lower than others and by adding movie bias terms to the global mean.</p>
<p>To implement the movie effects model, as in the baseline model, I first calculated μ (mu) as the overall mean rating from the training data. I then grouped the data by <code>movieId</code> and for each movie, calculated b_i (movie effect) as the average deviation from the global mean: b_i = mean(rating - μ) for each movie.</p>
<p>To generate predictions, I used <code>left_join</code> to join all the rows in the validation set with those in <code>movie_avgs</code> (which contained the movie effects) using the <code>movieId</code> to bring in the corresponding movie effect (b_i). To handle missing values for movie effect, which are movies not seen before, the NAs were replaced with 0 so that the prediction fell back to the global mean (μ). This ensured that every movie in the validation set gets a prediction, known movies got predictions based on their historical data and previously unseen movies received a safe fallback to the global mean. The final prediction formula was:</p>
<p><span class="math inline">\(predicted rating = μ + b_i\)</span></p>
<p>Where μ is the global mean rating (constant for all predictions) and b_i is the movie-specific effect (different for each movie).</p>
</section>
<section id="user-effects-model" class="level4">
<h4 class="anchored" data-anchor-id="user-effects-model"><strong>User effects model</strong></h4>
<p>This model improved upon the <a href="#baseline-model">Baseline model</a> by accounting for user-specific rating tendencies. This recognized that some users consistently rate movies higher/lower than others and so adds user bias terms to the global mean. For example, if a user typically rated movies 0.5 stars above average, their user effect (b_u) would be 0.5 and so future predictions for this user would add 0.5 to the global mean.</p>
<p>As previously, the global mean (μ) was used and, similarly to the [Movie effects model], the user effect (b_u) calculated as the average deviation from the global mean. To capture how much each user tended to rate above or below the average, for each user, I calculated the user effect as the average deviation from the global mean: b_u = mean(rating - μ) for each user.</p>
<p>To generate predictions I used <code>left_join</code> to join all the rows in the validation dataset with <code>user_avgs</code>, which contained each user’s bias term (b_u), this time matching on <code>userId</code> to provide the corresponding user effect. Missing values were handled as described previously. The final prediction formula was:</p>
<p><span class="math inline">\(predicted rating = μ + b_u\)</span></p>
<p>Where μ is the global mean rating (constant for all predictions) and b_u is the movie-specific effect (different for each user).</p>
</section>
<section id="combined-movie-and-user-effects-model" class="level4">
<h4 class="anchored" data-anchor-id="combined-movie-and-user-effects-model"><strong>Combined movie and user effects model</strong></h4>
<p>This model improved upon previous models by combining both movie and user biases, accounting for both movie quality and user rating tendencies. The model first calculated movie effects, then user effects and then joined both these to the validation data, first adding movie effects by matching on <code>movieId</code> and then adding user effects joining on <code>userId</code>. Missing values were handled as described previously. The final prediction formula was:</p>
<p><span class="math inline">\(predicted rating = μ + b_i + b_u\)</span></p>
<p>Where μ is global mean, b_i is the movie-specific effect and b_u is user-specific effect. For example if μ = 3.5, b_i = 0.3 (good movie), and b_u = 0.2 (generous rater), then the prediction would be 4.0 (3.5 + 0.3 + 0.2).</p>
</section>
<section id="regularized-model" class="level4">
<h4 class="anchored" data-anchor-id="regularized-model"><strong>Regularized model</strong></h4>
<p>This model improved upon the combined model by adding regularization to prevent overfitting. Regularization helps stabilize the model by penalizing large coefficients (extreme values), reducing their impact on predictions. To implement this model, I first modified the movie effects model to include the regularization parameter (λ, lambda) which controls the penalty for large coefficients, and then to the user effects model. λ values from 0 to 10 in steps of 0.25 (<code>seq(0, 10, 0.25)</code>) were tested for each model. I then joined both these to the validation data, first adding movie effects by matching on <code>movieId</code> and then adding user effects joining on <code>userId</code>. Missing values were handled as described previously. Note that regularization was applied during the calculation of each effect, not after combining them. The final prediction formula was:</p>
<p><span class="math inline">\(rating = μ + b_i + b_u\)</span> (but now both b_i and b_u are regularized)</p>
<p>Where: μ is global mean, b_i is movie-specific effect, b_u is user-specific effect</p>
<p>Predictions were made on the validation data and the RMSE computed for each lambda value (model tuning). The lambda value with the lowest RMSE was selected as the optimal lambda, representing the best balance between overfitting and maintaining predicitve power. This optimal lambda was then used to make predictions on the final holdout test set.</p>
</section>
<section id="regularized-genre-effects-model" class="level4">
<h4 class="anchored" data-anchor-id="regularized-genre-effects-model"><strong>Regularized genre effects model</strong></h4>
<p>The regularized genre effects model builds upon the previous models by incorporating and optimizing regularization for all three effects simultaneously: <code>movie (b_i)</code>, user <code>(b_u)</code>, and genre <code>(b_g)</code> effects. The model tests λ values from 0 to 10 in steps of 0.25 to find the optimal regularization strength that works best for all effects together. For each lambda value, it calculates regularized movie effects first <code>(b_i = sum(rating - μ)/(n_i + λ))</code>, then user effects accounting for movie effects <code>(b_u = sum(rating - μ - b_i)/(n_u + λ))</code>, and finally genre effects accounting for both movie and user effects <code>(b_g = sum(rating - μ - b_i - b_u)/(n_g + λ))</code>. The final prediction formula combines all these regularized effects:</p>
<p><span class="math inline">\(rating = μ + b_i + b_u + b_g\)</span> (but now both b_i, b_u and b_g are regularized)</p>
<p>Where: μ is global mean, b_i is movie-specific effect, b_u is user-specific effect, b_g is genre-specific effect.</p>
<p>This approach ensures that all effects are appropriately regularized with a single, optimized lambda value, preventing overfitting across all components of the model and providing more reliable predictions, especially for movies, users, or genres with few ratings.</p>
</section>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">3. Results</h2>
<section id="model-performance-comparison" class="level3">
<h3 class="anchored" data-anchor-id="model-performance-comparison">Model Performance Comparison</h3>
<p>The RMSE is a standard metric used to evaluate the accuracy of prediction models, particularly in recommendation systems. It measures the average magnitude of prediction errors minus the differences between values predicted by a model and the actual observed values.</p>
<ol type="1">
<li>For each prediction, calculate the error (predicted value minus actual value) 2. Square each error value (to make all values positive and penalize larger errors more heavily) 3. Calculate the mean of these squared errors 4. Take the square root of this mean</li>
</ol>
<p>The RMSE has several important characteristics:</p>
<ul>
<li><p>It has the same units as the quantity being estimated, making it interpretable</p></li>
<li><p>It disproportionately penalizes large errors due to the squaring operation</p></li>
<li><p>Lower RMSE values indicate better prediction accuracy</p></li>
<li><p>A perfect model would have an RMSE of 0</p></li>
</ul>
<p>In the context of movie recommendations, if the actual rating is 4 stars but the model predicts 3 stars, the error is 1. If we have many such predictions, the RMSE helps us understand the typical magnitude of our prediction errors across all ratings.</p>
</section>
<section id="best-model-analysis" class="level3">
<h3 class="anchored" data-anchor-id="best-model-analysis">Best Model Analysis</h3>
<ol type="1">
<li><p><strong>Baseline model</strong>: Starting with a simple model that predicts the global mean rating for all movies, I achieved an RMSE of 1.0599. This serves as the baseline for model comparison.</p></li>
<li><p><strong>Movie effects model</strong>: Adding movie-specific effects (accounting for some movies being generally rated higher or lower than others) improved the RMSE by 0.1162 points, representing a 11% improvement over the baseline.</p></li>
<li><p><strong>User effects model</strong>: Incorporating user-specific effects (accounting for individual rating tendencies) further improved predictions, reducing the RMSE by 0.0815 points.</p></li>
<li><p><strong>Combined effects model</strong>: The model combining both movie and user effects achieved an RMSE of 0.8659, demonstrating that considering both factors together provides better predictions than either factor alone.</p></li>
<li><p><strong>Regularized model</strong>: Adding regularization to prevent overfitting improved the model’s performance, particularly for movies and users with fewer ratings.</p></li>
</ol>
<p>The regularization parameter (λ) was tuned to find the optimal balance between fitting the training data and preventing overfitting. The plot below shows how the RMSE changes with different values of λ for both the initial model and the final model with all effects.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="combined_reg_plots.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
<p>The plots show that:</p>
<ul>
<li><p>For the initial model (movie and user effects only), the optimal λ value was 4.75</p></li>
<li><p>For the final model with all effects (including genre effects), the optimal λ value was 4.75</p></li>
<li><p>The curves demonstrate how different levels of regularization affect model performance:</p>
<ul>
<li><p>The minimum point in each curve represents the λ value that achieves the best balance</p></li>
<li><p>Values away from the minimum in either direction result in higher RMSE</p></li>
</ul></li>
<li><p>The vertical red lines mark the optimal λ values that achieve the lowest RMSE on the validation set</p></li>
</ul>
<p>In this analysis, the optimal lambda value for both the regularized movie+user effects model and the model including genre effects was the same. This indicates that, for this dataset, adding genre effects did not substantially change the regularization strength needed to prevent overfitting. While in theory, adding more parameters (such as genre effects) could require a different optimal lambda, in practice the minimum RMSE occurred at the same value for both models, suggesting the genre effect provided little additional improvement, which is confirmed by the modest improvement in performance</p>
<ol type="1">
<li><strong>Final regularized model with genre effects</strong>: The most sophisticated model, which includes movie effects, user effects, and genre preferences with regularization, achieved the best performance with an RMSE of 0.8649 on the validation set.</li>
</ol>
<p>The progressive improvement in RMSE across these models demonstrates the value of considering multiple factors in movie rating predictions. Each additional component (movie effects, user effects, genre effects) contributed to more accurate predictions, with the final regularized combined movie, user and genre effects model achieving the best performance.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-final-results" class="cell quarto-float anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-final-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Model performance comparison
</figcaption>
<div aria-describedby="tbl-final-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table table-striped table-hover do-not-create-environment cell table-sm small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">Method</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Baseline model</td>
<td style="text-align: right;">1.0599</td>
</tr>
<tr class="even">
<td style="text-align: left;">Movie Effects Model</td>
<td style="text-align: right;">0.9438</td>
</tr>
<tr class="odd">
<td style="text-align: left;">User Effects Model</td>
<td style="text-align: right;">0.9784</td>
</tr>
<tr class="even">
<td style="text-align: left;">Combined Effects Model</td>
<td style="text-align: right;">0.8659</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Regularized Model</td>
<td style="text-align: right;">0.8652</td>
</tr>
<tr class="even">
<td style="text-align: left;">Optimized Regularized Model with Genre Effects</td>
<td style="text-align: right;">0.8649</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Final Model on Holdout</td>
<td style="text-align: right;">0.8653</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p>To better understand how well our final model performs across different rating values, I created a boxplot comparing predicted ratings against actual ratings. This visualization helps us identify:</p>
<ol type="1">
<li>Where the model’s predictions were most accurate</li>
<li>If there are systematic biases in the predictions</li>
<li>How the prediction variance changes across different rating levels</li>
</ol>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="boxplot.png" class="img-fluid quarto-figure quarto-figure-left figure-img" width="414"></p>
</figure>
</div>
<p>The boxplot shows the distribution of predicted ratings (y-axis) for each actual rating value (x-axis). The boxes represent the interquartile range (IQR) of predictions, with the middle line showing the median predicted rating. The whiskers extend to show the full range of predictions, excluding outliers. This visualization is particularly useful for identifying:</p>
<ul>
<li><p>Whether our model tends to overpredict low ratings or underpredict high ratings</p></li>
<li><p>At which rating levels our predictions are most consistent (smaller boxes) or most variable (larger boxes)</p></li>
<li><p>Any systematic patterns in prediction errors across the rating scale</p></li>
<li><p>Discussion of feature importance (movie, user, and genre effects)</p></li>
<li><p>Examples of well-predicted vs.&nbsp;poorly-predicted ratings</p></li>
</ul>
<p>The x-axis shows the actual ratings (1-5 stars), where the y-axis shows the predicted ratings. Each box represents the distribution of predictions for a given actual rating.</p>
<p>The plot suggests that:</p>
<ul>
<li><p>The model tends to overpredict low ratings</p></li>
<li><p>The model tends to underpredict high ratings</p></li>
<li><p>The model is “conservative” in its predictions, being reluctant to predict extreme ratings (1 or 5 stars)</p></li>
</ul>
</section>
<section id="final-model-performance" class="level3">
<h3 class="anchored" data-anchor-id="final-model-performance">Final Model Performance</h3>
<ul>
<li>RMSE on final holdout test set: 0.8653</li>
<li>Comparison to project target (RMSE &lt; 0.86490)</li>
<li>Analysis of prediction distribution across different rating values</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">4. Conclusion</h2>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>My project has successfully developed a recommendation system utilizing the MovieLens dataset. Through careful analysis and iterative development, I determined that a regularized model incorporating movie, user, and genre effects delivered the strongest performance among all approaches tested. This final model achieved an RMSE of 0.8653 on the holdout test set, demonstrating robust predictive capabilities. My analysis showed that the biggest improvement in performance was the intorduciton of the combined effects model, which was slightly improved by the addition of the regularized model with genre effects.</p>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<p>Despite the final model’s strong performance, several limitations should be acknowledged. The current implementation does not account for temporal effects, such as how user preferences naturally evolve over time. Additionally, the model faces challenges with the cold start problem, where new users and movies would have limited prediction accuracy due to insufficient historical data. Another constraint is that the genre categories used in the dataset are quite broad and may not fully capture the nuanced content preferences that influence user ratings. Furthermore, it’s important to note that the dataset represents user behavior from a specific time period, which may limit its applicability to current viewing patterns.</p>
</section>
<section id="future-work" class="level3">
<h3 class="anchored" data-anchor-id="future-work">Future Work</h3>
<p>Moving forward, several promising avenues exist for enhancing the recommendation system. The incorporation of time-based features to better capture evolving user preferences over time. Exploring matrix factorization and latent factor models presents another opportunity for potential improvement. Additional refinements could include implementing content-based features using movie metadata and developing hybrid recommendation approaches that combine multiple techniques. If this were a real-world project, I could test the model on more recent datasets and incorporate additional features such as movie popularity, recency, and user activity metrics, which could provide valuable signals for improving recommendation quality.</p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-dslabs:2017" class="csl-entry" role="listitem">
<span>“Dslabs: Data Science Labs.”</span> 2017. The R Foundation. <a href="https://doi.org/10.32614/cran.package.dslabs">https://doi.org/10.32614/cran.package.dslabs</a>.
</div>
<div id="ref-irizarry2024" class="csl-entry" role="listitem">
Irizarry, Rafael A. 2024. <span>“Introduction to Data Science,”</span> July. <a href="https://doi.org/10.1201/9781003220923">https://doi.org/10.1201/9781003220923</a>.
</div>
<div id="ref-Re3data.Org2016" class="csl-entry" role="listitem">
Re3data.Org. 2016. <span>“Grouplens Datasets.”</span> <a href="https://doi.org/10.17616/R3ZH2D">https://doi.org/10.17616/R3ZH2D</a>.
</div>
<div id="ref-Zhuo2024" class="csl-entry" role="listitem">
Zhuo, Wei. 2024. <span>“MovieLens-10M.”</span> TIB. <a href="https://doi.org/10.57702/DFE6D5GM">https://doi.org/10.57702/DFE6D5GM</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>