---
title: "MovieLens Project Report"
author: "Kim Wager"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_width: 7
    fig_height: 5
    fig_caption: true
    highlight: tango
fontsize: 11pt
geometry: margin=1in
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        # Don't show code by default
  message = FALSE,     # Don't show messages
  warning = FALSE,     # Don't show warnings
  fig.align = "center" # Center figures
)
library(tidyverse)
library(knitr)
library(kableExtra)    # For nicer table formatting

# To knit final file to PDF use: rmarkdown::render("movie-lens-report.Rmd")
```

# Movie Recommendation System Report

## 1. Introduction

### Recommendation systems

Recommendation systems use ratings provided by users to generate personalized suggestions. Organizations like Amazon, with extensive product catalogs and customer bases, accumulate vast datasets of user ratings. These large datasets can be used to train models that predict how a specific user might rate items they haven't seen before. Items predicted to receive high ratings from a particular user are then recommended to them by the organization to drive sales.[@irizarry2024]

In the entertainment industry, Netflix uses recommendation systems to predict user movie ratings on a five-star scale, where one star indicates a poor film and five stars represents an excellent film.[@irizarry2024] The objective of this project was to develop a machine learning model to predict movie ratings.[@Re3data.Org2016]

### The MovieLens dataset

While Netflix's proprietary data are unavailable, the GroupLens research laboratory has developed an alternative open-source dataset called MovieLens with over 20 million ratings for more than 27,000 movies, provided by over 138,000 users.[@Re3data.Org2016] A subset of these data (MovieLens 10M) were made available for this project via the `dslabs package`.[@dslabs:2017]

The MovieLens 10M dataset contains approximately 10 million ratings applied to 10,000 movies by 72,000 users. The dataset includes user IDs, movie IDs, ratings (0.5-5 stars), timestamps, movie titles, and genres. Each movie can belong to multiple genres (e.g., "Comedy\|Romance\|Drama").[@Zhuo2024]

### Project goal

The goal of the project was to develop several movie recommendation models to develop a system that accurately predicts how users will rate movies they haven't seen yet. To compare the performance of the models I used the Root Mean Square Error (RMSE) metric.

-   Target: Achieve RMSE \< 0.86490 on the final holdout test set

### Key steps performed

-   Data preparation and partitioning into training, validation, and test sets
-   Progressive model building from simple baseline to complex models
-   Feature engineering to incorporate movie, user, and genre effects
-   Regularization to prevent overfitting
-   Final model evaluation on a separate holdout test set

## 2. Methods

### Data preparation

-   Downloaded and extracted MovieLens 10M dataset according to the instruction provided by edX
-   Joined ratings data with movie metadata
-   Created data partitions:
    -   90% for model development (`edx`)
    -   10% for final evaluation (`final_holdout_test`)
-   Further divided the development data:
    -   80% for training (`edx_train`)
    -   20% for validation

### Data Exploration and Visualization

-   Analysis of rating distribution (average ratings, variance)
-   Examination of movie popularity and its relationship with ratings
-   Analysis of user rating patterns
-   Investigation of genre impact on ratings
-   Visualization of key relationships and patterns in the data

### Insights from Exploration

-   Some movies are consistently rated higher/lower than average
-   Some users tend to rate more generously/critically than others
-   Certain genres receive systematically different ratings
-   Movies with few ratings show more extreme average ratings
-   Rating patterns differ across genres

### Modeling Approach

1.  **Baseline Model**: Global mean rating for all predictions
2.  **Movie Effects Model**: Adjusted for movie-specific deviations
3.  **User Effects Model**: Incorporated user rating tendencies
4.  **Combined Model**: Integrated both movie and user effects
5.  **Regularized Model**: Added regularization to prevent overfitting
6.  **Genre Effects Model**: Included genre-specific biases
7.  **Genre-Specific User Effects**: Analyzed user preferences by genre

## 3. Results

### Model Performance Comparison

The RMSE is a standard metric used to evaluate the accuracy of prediction models, particularly in recommendation systems. It measures the average magnitude of prediction errors minus the differences between values predicted by a model and the actual observed values.

1\. For each prediction, calculate the error (predicted value minus actual value)
2. Square each error value (to make all values positive and penalize larger errors more heavily)
3. Calculate the mean of these squared errors
4. Take the square root of this mean

The RMSE has several important characteristics:

-   It has the same units as the quantity being estimated, making it interpretable

-   It disproportionately penalizes large errors due to the squaring operation

-   Lower RMSE values indicate better prediction accuracy

-   A perfect model would have an RMSE of 0

In the context of movie recommendations, if the actual rating is 4 stars but the model predicts 3 stars, the error is 1. If we have many such predictions, the RMSE helps us understand the typical magnitude of our prediction errors across all ratings.

Mathematically, RMSE is represented as:

$$\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$$
Where:

-   $n$ is the number of predictions
-   $y_i$ is the actual observed value
-   $\hat{y}_i$ is the predicted value

-   Baseline Model RMSE: \[value\]
-   Movie Effects Model RMSE: \[value\]
-   User Effects Model RMSE: \[value\]
-   Combined Effects Model RMSE: \[value\]
-   Regularized Model RMSE: \[value\]
-   Genre Effects Model RMSE: \[value\]
-   Genre-Specific User Effects RMSE: \[value\]

### Best Model Analysis

-   Detailed examination of the best-performing model
-   Analysis of regularization parameter (lambda) tuning
-   Visualization of prediction accuracy vs. actual ratings
-   Discussion of feature importance (movie, user, and genre effects)
-   Examples of well-predicted vs. poorly-predicted ratings

### Final Model Performance

-   RMSE on final holdout test set: \[value\]
-   Comparison to project target (RMSE \< 0.86490)
-   Analysis of prediction distribution across different rating values

## 4. Conclusion

### Summary

-   The project successfully developed a recommendation system using the MovieLens dataset
-   A regularized model incorporating movie, user, and genre effects performed best
-   The final model achieved an RMSE of \[value\] on the holdout test set
-   Key factors affecting prediction accuracy were \[list factors\]

### Limitations

-   The model doesn't account for temporal effects (e.g., changing user preferences over time)
-   New users and movies (cold start problem) would have limited prediction accuracy
-   Genre categories are broad and may not capture nuanced content preferences
-   The dataset represents user behavior from a specific time period

### Future Work

-   Incorporate time-based features to capture evolving preferences
-   Explore matrix factorization and latent factor models
-   Implement content-based features using movie metadata
-   Develop hybrid recommendation approaches
-   Test the model on more recent datasets
-   Incorporate additional features like movie popularity, recency, and user activity