---
title: "MovieLens Recommendation System"
author: "Kim Wager"
date: "2024-01-10"
output: 
  html_document:
    theme: cosmo
    highlight: tango
    df_print: paged
    mathjax: default
---

## Create the datasets as instructed

```{r}
##########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                       stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

<!--# Ref. http://rafalab.dfci.harvard.edu/dsbook/large-datasets.html -->

For model development, instead of using the entire `edx` dataset, I will split it into two parts:
1.  `edx_train` to develop the model
2.  `validation` to tune and evaluate intermediate model performance

```{r}
# Create validation set from edx for model development
set.seed(1, sample.kind="Rounding")
validation_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
validation <- edx[validation_index,]
edx_train <- edx[-validation_index,]
```
## Baseline model

First, we implement the simple baseline model and calculate the RMSE. This baseline model makes the same prediction (the overall average rating) for every movie, regardless of the specific movie or user. While this is obviously not very sophisticated, it gives us:

-   A starting point for measuring improvement
-   A simple implementation to verify the RMSE calculation function
-   A benchmark that any more complex model should beat

### Step 1: Calculate the mean rating (μ)

This calculates μ (mu), which represents the average of all ratings in the training dataset (`edx`). We use this as our simplest possible prediction - assuming every movie would be rated at this average value.

```{r}
# Calculate the overall mean rating from training data
mu <- mean(edx_train$rating)
print(paste("Overall mean rating:", round(mu, 4)))
```

### Step 2: Create predictions for validation set

This creates a vector of predictions by repeating the mean rating (μ) for each row in the test dataset. The `rep()` function repeats the value, and `nrow(validation)` tells it how many times to repeat.

```{r}
# Create predictions using just the mean (mu) from the previous step
predictions <- rep(mu, nrow(validation))
```

### Step 3: Define RMSE function and calculate error

The RMSE function calculates the Root Mean Square Error, which measures how far the predictions deviate from the actual ratings. It works by:

1.  Taking the difference between true and predicted ratings
2.  Squaring these differences
3.  Calculating the mean of the squared differences
4.  Taking the square root of that mean

```{r}
# Define RMSE function
# Takes two parameters: true ratings (edx dataset) and predicted ratings (final_holdout_test dataset)
# Returns a single number representing prediction accuracy (lower is better)
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

# Calculate RMSE
# Print the mean rating with 4 decimal places for clarity
# paste() combines text and numeric value into a single string
naive_rmse <- RMSE(validation$rating, predictions)
print(paste("RMSE for baseline model:", round(naive_rmse, 4)))
```

### Step 4: Create results table

Here we create a data frame to store the results. This is used to help us compare different models as we develop more complex ones. The method column describes the approach used and the RMSE column stores the error value for that method.

```{r}
# Create results table
rmse_results <- tibble(
  Method = "Baseline model",
  RMSE = naive_rmse
)

knitr::kable(rmse_results, digits = 4, 
             caption = "Baseline model performance")
```

## Modelling movie effects

This baseline model makes the same prediction (the overall average rating from the training dataset) for every movie, regardless of the specific movie or user. While this is obviously not very sophisticated, it gives us:

1.  A starting point for measuring improvement

2.  A simple implementation to verify our RMSE calculation function

3.  A benchmark that any more complex model should beat

The key insight from this baseline model is that we get an **RMSE of 1.06**, this means our predictions are off by about 1.06 stars on average.

In the real-world, of course, not all movies are created equal. Some movies are inherently more liked or disliked than others, which is reflected in the value of their rating. The movie effect captures the average rating difference for each specific movie, which should improve prediction accuracy.


```{r}
# Calculate global mean (as previoulsy)
mu <- mean(edx_train$rating)

# Calculate movie effect (b_i)
movie_avgs <- edx_train %>%
   group_by(movieId) %>%
   summarize(b_i = mean(rating - mu))

# Predict ratings using global mean + movie effect
predicted_ratings <- validation %>%
  left_join(movie_avgs, by = "movieId") %>%
  mutate(
    # Replace NA movie effects with 0 (global mean), in case some movies are in the validation set but not the training set
    b_i = ifelse(is.na(b_i), 0, b_i),
    pred = mu + b_i
  ) %>% 
  pull(pred)

# Calculate Root Mean Square Error (RMSE)
movie_effect_rmse <- RMSE(validation$rating, predicted_ratings)

# Print results
print(paste("Movie Effects RMSE:", movie_effect_rmse))

# Caluclate improvement over naive RMSE
improvement <- naive_rmse - movie_effect_rmse
improvement_percentage_movie_effects <- (naive_rmse - movie_effect_rmse) / naive_rmse * 100

# Add to results table
rmse_results <- bind_rows(rmse_results,
                         tibble(Method = "Movie Effects Model",
                                RMSE = movie_effect_rmse))
```

The 11% (10.96) reduction in RMSE when accounting for movie effects demonstrates that:

-   Movie-specific effects capture meaningful variation in ratings
-   Not all movies are rated equally
-   Accounting for movie-to-movie differences improves predictions

Next, I need to account for user effects because not all users rate movies the same way. For example, some users may be:

-   More critical (tend to give lower ratings)

-   More generous (tend to give higher ratings)

-   More consistent in their ratings

-   More variable in their ratings

```{r}
# Calculate global mean (as previoulsy)
mu <- mean(edx_train$rating)

# Calculate user effects
user_avgs <- edx_train %>%
   group_by(userId) %>%
   summarize(b_u = mean(rating - mu))

# Predict ratings
predicted_ratings <- validation %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(
    # Handle missing users
    b_u = ifelse(is.na(b_u), 0, b_u),
    pred = mu + b_u
  ) %>% 
  pull(pred)

# Calculate RMSE
user_effect_rmse <- RMSE(validation$rating, predicted_ratings)

print(paste("User Effects RMSE:", user_effect_rmse))

# Caluclate improvement over naive RMSE
improvement <- naive_rmse - user_effect_rmse
improvement_percentage_user_effects <- (naive_rmse - user_effect_rmse) / naive_rmse * 100

# Add to results
rmse_results <- bind_rows(rmse_results,
                         tibble(Method = "User Effects Model",
                                RMSE = user_effect_rmse))

# Display results table
knitr::kable(rmse_results, digits = 4,
             caption = "Model performance on validation set")                              
```

The 8% (7.69) reduction in RMSE when accounting for user effects demonstrates:
- individual user rating patterns significantly impact movie ratings
- some users consistently rate movies higher or lower than average, showing systematic bias in rating behavior
- this improvement indicates that accounting for user-specific rating tendencies leads to better predictions than using just the global mean rating.

So far, I have used three independent modelling approaches; however, this does not capture real-world complexity because there may be interaction effects between movie and user. To do this I take the user's rating, subtract the movie's typical rating and the overall average rating: this shows if a user tends to rate higher or lower than the others. 


```{r}
# Combined movie and user effects model
# Calculate global mean (as previously)
mu <- mean(edx_train$rating)

# Calculate movie effects (as previously)
movie_avgs <- edx_train %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

# Calculate user effects accounting for movie effects (join movie effects with training data)

user_avgs <- edx_train %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

```

To make predictions for each movie-user pair (the joined data), I take the overall average rating and add both the movie and user's typical deviation from the average.

```{r}
# Make predictions on validation set
predicted_ratings <- validation %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(
    b_i = ifelse(is.na(b_i), 0, b_i),
    b_u = ifelse(is.na(b_u), 0, b_u),
    pred = mu + b_i + b_u
  ) %>%
  pull(pred)

# Calculate RMSE
combined_effect_rmse <- RMSE(validation$rating, predicted_ratings)
print(paste("Combined Effects RMSE (validation):", combined_effect_rmse))

# Caluclate improvement over naive RMSE
improvement <- naive_rmse - combined_effect_rmse
improvement_percentage_combined_effects <- (naive_rmse - combined_effect_rmse) / naive_rmse * 100

# Add to results
rmse_results <- bind_rows(rmse_results,
                         tibble(Method = "Combined Effects Model",
                                RMSE = combined_effect_rmse))

# Display results table
knitr::kable(rmse_results, digits = 4,
             caption = "Model performance on validation set")
```

The 18% (18.30) reduction in RMSE when accounting for combined movie and user effects captures both how movies typically deviate from the average and how users typically rate compared with other users. It improves predictions by accounting for both movie quality and user rating behavior simultaneously. For example, if a typically harsh critic (negative user effect) rates a highly-rated movie (positive movie effect), the model can balance these opposing factors for a more accurate prediction. The improvement over individual movie and user effects models demonstrates that these factors are not independent. 

Howver, the RMSE for the combined effects model is still 0.87, which is relatively high. To help reduce this further, we can use regularization, which adds a penalty term (λ lambda) to prevent overconfidence in ratings that are formed using small sample sizes. The optimal lamda term is found using cross-validation. 

What this does is for movies or users with few ratings, pulls estimates closer to the mean, while allowing larger deviations from the mean for movies or users with many ratings. This therefore prevents overfitting to the training data. 


```{r}
# Regularized movie and user effects model
# Try different lambda values
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  # Regularized movie effects
  movie_reg_avgs <- edx_train %>%
    group_by(movieId) %>%
    summarize(
      b_i = sum(rating - mu)/(n() + l),
      n_i = n()
    )
  
  # Regularized user effects
  user_reg_avgs <- edx_train %>%
    left_join(movie_reg_avgs, by = "movieId") %>%
    group_by(userId) %>%
    summarize(
      b_u = sum(rating - mu - b_i)/(n() + l),
      n_u = n()
    )
  
  # Make predictions
  predicted_ratings <- validation %>%
    left_join(movie_reg_avgs, by = "movieId") %>%
    left_join(user_reg_avgs, by = "userId") %>%
    mutate(
      b_i = ifelse(is.na(b_i), 0, b_i),
      b_u = ifelse(is.na(b_u), 0, b_u),
      pred = mu + b_i + b_u
    ) %>%
    pull(pred)
  
  return(RMSE(validation$rating, predicted_ratings))
})

# Find optimal lambda
lambda <- lambdas[which.min(rmses)]
print(paste("Optimal lambda:", lambda))

# Plot RMSE vs lambda
qplot(lambdas, rmses) +
  geom_line() +
  xlab("Lambda") +
  ylab("RMSE") +
  ggtitle("RMSE vs Regularization Parameter")

# Add results
rmse_results <- bind_rows(rmse_results,
                         tibble(Method = "Regularized combined effects model",
                                RMSE = min(rmses)))

# Display updated results
knitr::kable(rmse_results, digits = 4,
             caption = "Model performance on validation set")


```

The performance of the regularized combined model is only slightly better than the combined model. It may be that there is a genre effect, so we can calculate regularized genre effects and combine it with the regularized movie and user effects.


```{r}
# Regularized genre, movie and user effects model
# Split genres and calculate genre effects
genre_avgs <- edx_train %>%
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(
    b_g = sum(rating - mu)/(n() + lambda),
    n_g = n()
  )

# Add genre effects to predictions
predicted_ratings <- validation %>%
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(user_reg_avgs, by = "userId") %>%
  separate_rows(genres, sep = "\\|") %>%
  left_join(genre_avgs, by = "genres") %>%
  group_by(userId, movieId) %>%
  summarize(
    pred = first(mu) + first(b_i) + first(b_u) + mean(b_g, na.rm = TRUE)
  ) %>%
  pull(pred)

# Calculate RMSE with genres
genre_rmse <- RMSE(validation$rating, predicted_ratings)
print(paste("RMSE with genre effects:", genre_rmse))

# Add to results
rmse_results <- bind_rows(rmse_results,
                         tibble(Method = "Regularized with Genres",
                                RMSE = genre_rmse))

# Calculate global mean once
mu <- mean(edx_train$rating)

# Find optimal lambda for genre model
lambdas <- seq(0, 10, 0.25)
genre_rmses <- sapply(lambdas, function(l){
  
  # Calculate all effects within lambda loop
  movie_avgs <- edx_train %>%
    group_by(movieId) %>%
    summarize(
      b_i = sum(rating - mu)/(n() + l)
    )
  
  user_avgs <- edx_train %>%
    left_join(movie_avgs, by = "movieId") %>%
    group_by(userId) %>%
    summarize(
      b_u = sum(rating - mu - b_i)/(n() + l)
    )
    
  genre_avgs <- edx_train %>%
    separate_rows(genres, sep = "\\|") %>%
    group_by(genres) %>%
    summarize(
      b_g = sum(rating - mu)/(n() + l)
    )
  
  predicted_ratings <- validation %>%
    left_join(movie_avgs, by = "movieId") %>%
    left_join(user_avgs, by = "userId") %>%
    separate_rows(genres, sep = "\\|") %>%
    left_join(genre_avgs, by = "genres") %>%
    group_by(userId, movieId) %>%
    summarize(
      pred = first(mu) + first(b_i) + first(b_u) + mean(b_g, na.rm = TRUE)
    ) %>%
    pull(pred)
  
  return(RMSE(validation$rating, predicted_ratings))
})


# Find optimal lambda
lambda_genre <- lambdas[which.min(genre_rmses)]
print(paste("Optimal lambda for genre model:", lambda_genre))

# Plot RMSE vs lambda
ggplot(data.frame(lambda = lambdas, rmse = genre_rmses), aes(x = lambda, y = rmse)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE vs Lambda (Genre Model)",
       x = "Lambda",
       y = "RMSE") +
  theme_minimal()

# Add best result to table
rmse_results <- bind_rows(rmse_results,
                         tibble(Method = "Optimized Genre Model",
                                RMSE = min(genre_rmses)))

# Display updated results
knitr::kable(rmse_results, digits = 4,
             caption = "Model performance on validation set")

```